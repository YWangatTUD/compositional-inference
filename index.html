<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Compositional Scene Understanding through Inverse Generative Modeling.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Compositional Scene Understanding through Inverse Generative Modeling</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-3593LNYVK9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-3593LNYVK9');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="bootstrap-grid.css">
  <link rel="stylesheet" href="slideshow.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="load-mathjax.js" async></script>
  <script src="slideshow.js" defer></script>

</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://github.com/jsu27">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item"
              href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/">
              Composable Diffusion
            </a>
            <a class="navbar-item" href="https://energy-based-model.github.io/decomp-diffusion/">
              Decomposed Diffusion
            </a>
            <a class="navbar-item" href="https://energy-based-model.github.io/comet/">
              COMET
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Compositional Scene Understanding through Inverse Generative Modeling</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://ywangattud.github.io/website/">Yanbo Wang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=dboVuDYAAAAJ&hl=en">Justin Dauwels</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://yilundu.github.io/">Yilun Du</a><sup>2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>TU Delft,</span>
              <span class="author-block"><sup>2</sup>Harvard University</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><strong>ICML 2025</strong></span>
            </div>

            <!--
            <div class="column has-text-centered">
              <div class="publication-links">

                <span class="link-block">
                  <a href="https://openreview.net/pdf/6a53a10d59984c4ecee11dd1d94e05077e6b3c81.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.19298" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
 
                <span class="link-block">
                  <a href="https://github.com/jsu27/decomp_diffusion"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
          -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body" style="text-align:center">
        <video id="teaser" autoplay muted loop playsinline height="100%" style="display: block; margin: 0 auto;">
          <source src="materials/figures/comp_infer_teaser_animation_new.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </section>
  <hr>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We explore how generative models can be used not only to synthesize
              visual content but also to understand the properties of a scene given a natural image.
              We formulate scene understanding as an inverse generative modeling problem,
              where we seek to find conditional parameters of a visual generative model to best
              fit a given natural image. To enable this procedure to infer scene structure from images
              substantially different than those seen during training, we further propose to build
              this visual generative model compositionally from smaller models over pieces of
              a scene. We illustrate how this procedure enables us to infer the set of objects in a
              scene, enabling robust generalization to new test scenes with an increased number of
              objects of new shapes. We further illustrate how this enables us to infer global scene
              factors, likewise enabling robust generalization to new scenes. Finally, we illustrate
              how this approach can be directly applied to existing pretrained text-to-image generative
              models for zero-shot multi-object perception.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <hr>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <!-- <p>
            <p>We discover a set of compositional concepts given a dataset of unlabeled images. Score functions representing each concept $\{c^1, \dots, c^K\}$ are composed together to form a compositional score function that is trained to denoise images. The inferred concepts can be used to generate new images.
          </p> -->

            <p><strong>Compositional generative modeling for training.</strong> In a visual domain, given a set of conditioned concepts $\boldsymbol{c}^1, ..., \boldsymbol{c}^K$, we construct a generative model that can accurately represent the probability distribution
               $ p(\boldsymbol{x}| \boldsymbol{c}^1, \boldsymbol{c}^2, ..., \boldsymbol{c}^K)$
                over the space of images $\boldsymbol{x}$.
              To enable effective compositional generalization to a larger number of visual concepts, we further factorize
$p(\boldsymbol{x}|\boldsymbol{c}^1, \ldots, \boldsymbol{c}^K) \propto \prod_{k=1}^{K} p(\boldsymbol{x}|\boldsymbol{c}^{k})$

              with each $p(\boldsymbol{x}|\boldsymbol{c}^k)$ represented as a diffusion model $\epsilon_\theta(\boldsymbol{x}^t, t|\boldsymbol{c}^k)$, and train a composition of score functions with the denoising diffusion objective
    $\mathcal{L}_{\theta} =  \mathbb{E}_{\boldsymbol{x}, \mathbf{\epsilon}, t}\|\mathbf{\epsilon}- \sum_{k=1}^K \epsilon_\theta(\boldsymbol{x}^t, t|\boldsymbol{c}^k)\|^2$.
              </p>
            <p>
             <strong>Inverse generative modeling for inference.</strong> Once the generative model is trained, our approach can find a set of visual concepts that maximize the log-likelihood of the observed image $\boldsymbol{x}$ by solving
$\hat{\boldsymbol{c}}^1, \ldots, \hat{\boldsymbol{c}}^K = \text{argmin}_{\boldsymbol{c}^1, \ldots, \boldsymbol{c}^K} \mathbb{E}_{\mathbf{\epsilon}, t}\|\mathbf{\epsilon}- \sum_{k=1}^K \epsilon_\theta(\boldsymbol{x}^t, t|\boldsymbol{c}^k)\|^2$.
            </p>


            <!-- <video autoplay muted loop playsinline height="100%">
            <source src="materials/figures/method_animation.mp4" type="video/mp4">
          </video> -->
            <img src="materials/figures/approach_overview.png" alt="overview." />

            <!-- Composition between factors is achieved by sampling images from a composed diffusion distribution. Similar to composition between energy functions, this allows individual factors to represent both global and local concepts, and enables the recombination of concepts across different models and datasets. However, unlike the underlying energy decomposition objective of COMET, Decomp Diffusion may directly be trained through denoising, a stable and less expensive learning objective, and leads to higher resolution images. -->

            <!-- \def\vx{{\bm{x}}} -->

            <br>
          </div>
        </div>
      </div>
    </div>
          <hr>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Results</h2>
          <div class="content has-text-justified">
            <p>
              Below, we illustrate additional examples to demonstrate how inverse generative modeling coupled with compositionality can not only infer
              concepts from images but also generalize effectively to scenes more complex than seen during training.
            </p>
          </div>
        </div>
      </div>

      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h3 class="title">Infer Object Locations</h3>
          <p>
            Our approach can infer local factors (such as object coordinates) and object number from a test image,
            and effectively generalize to scenes containing a larger number of objects and more complex objects than those seen during training.
          </p>
          <br>
          <div class="content has-text-justified has-text-centered">
            <!-- </div> -->
            <div class="column is-full-width">
              <div style="display: flex; gap: 10px;">
              <img src="materials/figures/obj_location_id.png" style="width: 50%;" alt="Global Decomposition." />
                <img src="materials/figures/object_num.jpg" style="width: 50%;" alt="Image 2">
              </div>
              <p class="caption"><b>In-distribution Object Discovery.</b> We train our model with CLEVR images containing 3-5 objects. <strong>On the left</strong>, given an in-distribution image (also containing 3-5 objects), our approach accurately identifies object coordinates.
                <strong>On the right</strong>, we illustrate our approach can determine object number by selecting a number with the lowest denoising error.</p>

              <img src="materials/figures/obj_location_ood.png" alt="Global Decomposition." />
              <p class="caption"><b>Out-of-distribution Object Discovery. </b> Object perception results on out-of-distribution images: CLEVR images with 6-8 objects (<strong>Left</strong>) or CLEVRTex images with 6-8 objects (<strong>Right</strong>). Our model is trained with CLEVR images containing 3-5 objects. During inference time, given an out-of-distribution image that is substantially different from training data, our proposed approach can still infer the object positions accurately. In contrast, all baseline models predict object locations that significantly deviate from the ground truth.</p>
            </div>

          </div>

        </div>
      </div>
    </div>
  </section>
  <hr>



  <section class="section">
    <div class="container is-max-desktop">

      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Infer Facial Attributes</h2>
          <p>
            Our approach can also infer global factors, such as facial attributes, from a test image and reliably generalize to images that differ substantially from training data.
          </p>
          <br>
          <div class="content has-text-justified has-text-centered">
            <div class="column is-full-width">
              <img src="materials/figures/facial_attribute.png" alt="Local Decomposition." />
              <p class="caption"><b>In-Distribution and Out-of-Distribution Facial Feature Prediction.</b>Facial feature prediction results for in-distribution (<strong>Left</strong>) and out-of-distribution (<strong>Right</strong>) CelebA images.
                Our model is trained on female faces from CelebA. During inference, our model can accurately predict facial features consistent with the ground truth for both in-distribution female faces and out-of-distribution male faces.</p>
            </div>
          </div>

          <!-- <br /> -->
        </div>
      </div>
    </div>
  </section>
  <hr>


  <section class="section">
    <div class="container is-max-desktop">

      <!-- Animation. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Infer Object Categories</h2>
          <p>
            Finally, we show how our approach can leverage pretrained diffusion models, such as Stable Diffusion, for zero-shot multi-object perception tasks without requiring any additional training.
          </p>
          <br>
          <div class="content has-text-justified has-text-centered">
            <div class="column is-full-width">
              <img src="materials/figures/obj_categories.png" width="600" height="400" alt="Multi-Modal Decomposition." style="display: block; margin: 0 auto;" />
              <p class="caption" style="text-align: center;"><b>Zero-Shot Multi-Object Perception on Real-World Images.</b> </p>
            </div>
          </div>

      </div>
    </div>
  </section>

  <hr>





  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Projects</h2>
          Check out a list of our related papers on compositional generation and energy based models. A full list can be
          found <a href="https://energy-based-model.github.io/Energy-based-Model-MIT/">here</a>!
          <br>
          <br>
          <br>

          <div class="row vspace-top">
            <div class="col-sm-3">
              <video width="100%" playsinline="" autoplay="" preload="" muted="">
                <source src="materials/related/concept_discovery.m4v" type="video/mp4">
              </video>
            </div>
            <div class="col-sm-9">
              <div class="paper-title">
                <a href="https://energy-based-model.github.io/unsupervised-concept-discovery/">Unsupervised
                  Compositional Concepts Discovery with Text-to-Image Generative Models</a>
              </div>
              <div>
                We present an unsupervised approach to discover generative concepts from a collection of images. We show
                how such generative concepts can accurately represent the content of images, be recombined and composed
                to generate new artistic and hybrid images, and be used as a representation for downstream
                classification tasks.

              </div>
            </div>
          </div>
          <br>
          <br>



          <div class="row vspace-top">
            <div class="col-sm-3">
              <video width="100%" playsinline="" autoplay="" preload="" muted="">
                <source src="materials/related/recyle.m4v" type="video/mp4">
              </video>
            </div>
            <div class="col-sm-9">
              <div class="paper-title">
                <a href="https://energy-based-model.github.io/reduce-reuse-recycle/">Reduce, Reuse, Recycle:
                  Compositional Generation with Energy-Based Diffusion Models and MCMC</a>
              </div>
              <div>
                We propose new samplers, inspired by MCMC, to enable successful compositional generation. Further, we
                propose an energy-based parameterization of diffusion models which enables the use of new compositional
                operators and more sophisticated, Metropolis-corrected samplers.
              </div>
            </div>
          </div>
          <br>


          <div class="row vspace-top">
            <div class="col-sm-3">
              <video width="100%" playsinline="" autoplay="" preload="" muted="">
                <source src="materials/related/comet.mp4" type="video/mp4">
              </video>
            </div>
            <div class="col-sm-9">
              <div class="paper-title">
                <a href="https://energy-based-model.github.io/comet/">Unsupervised Learning of Compositional Energy
                  Concepts</a>
              </div>
              <div>
                We propose COMET, which discovers and represents concepts as separate energy functions, enabling us to
                represent both global concepts as well as objects under a unified framework. COMET discovers energy
                functions through recomposing the input image, which we find captures independent factors without
                additional supervision.
              </div>
            </div>
          </div>
          <br>


          <div class="row vspace-top">
            <div class="col-sm-3">
              <video width="100%" playsinline="" autoplay="" preload="" muted="">
                <source src="materials/related/teaser_glide.mp4" type="video/mp4">
              </video>
            </div>
            <div class="col-sm-9">
              <div class="paper-title">
                <a
                  href="https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/">Compositional
                  Visual Generation with Composable Diffusion Models</a>
              </div>
              <div>
                We present a method to compose different diffusion models together, drawing on the close connection of
                diffusion models with EBMs. We illustrate how compositional operators enable
                the ability to composing multiple sets of objects together as well as generate images subject to
                complex text prompts.
              </div>
            </div>
          </div>
          <br>


          <div class="row vspace-top">
            <div class="col-sm-3">
              <video width="100%" playsinline="" autoplay="" preload="" muted="">
                <source src="materials/related/clevr_teaser.mp4" type="video/mp4">
              </video>
            </div>
            <div class="col-sm-9">
              <div class="paper-title">
                <a href="https://composevisualrelations.github.io/">Learning to Compose Visual Relations</a>
              </div>
              <div>
                The visual world around us can be described as a structured set of objects and their associated
                relations. In this work, we propose to represent each relation as an unnormalized density (an
                energy-based model), enabling us to compose separate relations in a factorized manner. We show that such
                a factorized decomposition allows the model to both generate and edit scenes that have multiple sets of
                relations more faithfully.
              </div>
            </div>
          </div>
          <br>

          <div class="row vspace-top">
            <div class="col-sm-3">
              <div class="move-down">
                <img src="materials/related/comp_cartoon.png" class="img-fluid" alt="comp_carton" style="width:100%">
              </div>
            </div>
            <div class="col-sm-9">
              <div class="paper-title">
                <a href="https://energy-based-model.github.io/compositional-generation-inference/">Compositional Visual
                  Generation with Energy Based Models</a>
              </div>
              <div>
                We present a set of compositional operators that enable EBMs to exhibit <b>zero-shot compositional</b>
                visual generation, enabling us to compose visual concepts
                (through operators of conjunction, disjunction, or negation) together in a zero-shot manner.
                Our approach enables us to generate faces given a description
                ((Smiling AND Female) OR (NOT Smiling AND Male)) or to combine several different objects together.
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- 
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content column is-full-width ">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{
}</code></pre>
  </div>
</section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/jsu27/decomp_diffusion" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8 has-text-centered">
          <div class="content">
            <p>
              This website is forked from the
              <a href="https://nerfies.github.io/">Nerfies</a>
              <a href="https://github.com/nerfies/nerfies.github.io">source code</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
